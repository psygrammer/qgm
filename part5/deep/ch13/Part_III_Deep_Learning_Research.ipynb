{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III Deep Learning Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 싸이그래머 / QGM : 파트 5 - 딥러닝 [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">This part of the book describes the more ambitious and advanced approaches to deep learning, currently pursued by the research community</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=\"red\">In the previous parts of the book</font>, we have shown how to solve <font color=\"red\">supervised learning problems</font>\n",
    "    - how to learn to map one vector to another, \n",
    "        - given enough examples of the mapping.\n",
    "* <font color=\"red\">Not all problems we might want to solve fall into this category(=supervised learning)</font>. \n",
    "     - other prloblems\n",
    "         - generate new examples, \n",
    "         - determine how likely some point is, \n",
    "         - handle missing values \n",
    "         - take advantage of a large set of unlabeled examples \n",
    "         - examples from related tasks. \n",
    "* A <font color=\"red\">shortcoming</font> of the current state of the art for industrial applications is that \n",
    "    - our learning algorithms require \n",
    "        - <font color=\"red\">large amounts of supervised data</font> to achieve good accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Many deep learning algorithms have been designed to tackle unsupervised learning problems, \n",
    "    - but none have truly solved the problem in the same way \n",
    "        - that deep learning has largely solved the supervised learning problem for a wide variety of tasks. \n",
    "* In this part of the book, we describe the existing approaches to unsupervised learning and some of the popular thought about how we can make progress in this ﬁeld."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### diﬃculties "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A central cause of the difficulties with unsupervised learning is the <font color=\"red\">high dimensionality</font> of the random variables being modeled. \n",
    "* This brings two distinct challenges: \n",
    "    - a <font color=\"blue\">statistical challenge</font> and \n",
    "    - a <font color=\"blue\">computational challenge</font>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### statistical challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The statistical challenge regards generalization:\n",
    "    - the number of conﬁgurations we may want to distinguish can \n",
    "        - grow exponentially with the number of dimensions of interest, and \n",
    "        - this quickly becomes much larger \n",
    "            - than the number of examples \n",
    "                - one can possibly have (or use with bounded computational resources). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### computational challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The computational challenge associated with high-dimensional distributions arises \n",
    "    - because many algorithms for learning or using a trained model (especially those based on estimating an explicitprobability function) \n",
    "    - involve intractable computations that \n",
    "    - grow exponentially with the number of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### probabilistic models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With probabilistic models, this computational challenge arises from \n",
    "    - the need to perform <font color=\"red\">intractable inference</font> or\n",
    "    - simply from the need to <font color=\"red\">normalize the distribution</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intractable inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Intractable inference: inference is discussed mostly in chapter 19. \n",
    "* It regards the question of guessing the probable values of some variables $a$, given other variables $b$, with respect to a model that captures the joint distribution over $a$, $b$ and $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intractable normalization constants (the partition function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Intractable normalization constants (the partition function): the partitionfunction is discussed mostly in chapter 18. \n",
    "* Normalizing constants of proba-bility functions come up in inference (above) as well as in learning. \n",
    "* Many probabilistic models involve such a normalizing constant. \n",
    "* Unfortunately,learning such a model often requires computing the gradient of the logarithm of the partition function with respect to the model parameters.\n",
    "* That computation is generally as intractable as computing the partition function itself. \n",
    "* Monte Carlo Markov chain (MCMC) methods (chapter 17) are often used to deal with the partition function (computing it or its gradient).\n",
    "* Unfortunately, MCMC methods suﬀer when the modes of the model distribution are numerous and well-separated, especially in high-dimensional spaces(section 17.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">One way to confront these intractable computations is to approximate them</font>, and many approaches have been proposed as discussed in this third part of the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generative models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Another interesting way, also discussed here, would be to avoid these intractable computations altogether by design, and methods that do not require such computations are thus very appealing. \n",
    "* Several generative models have been proposed in recent years, with that motivation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료\n",
    "[1] Part III: Deep Learning Research (DeepLearning Book) - http://www.deeplearningbook.org/contents/part_research.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
