{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatper 13. Linear Factor Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 싸이그래머 / QGM : 파트 5 - 딥러닝 [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* 13.1 Probabilistic PCA and Factor Analysis\n",
    "* 13.2 Independent Component Analysis (ICA)\n",
    "* 13.3 Slow Feature Analysis\n",
    "* 13.4 Sparse Coding\n",
    "* 13.5 Manifold Interpretation of PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probabilistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the research frontiers in deep learning involve building a probabilistic model of the input : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P_{model}(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### probabilistic inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a model can, in principle, use probabilistic inference to predict any of the variables in its environment given any of the other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### latent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these models also have latent variables : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h$, with $P{model}(x)$ = $E_{h}P_{model}(x|h)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributed representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* These latent variables provide another means of representing the data. \n",
    "* Distributed representations based on latent variables \n",
    "    - can obtain all of the advantages of representation learning \n",
    "        - that we have seen with deep feedforward and recurrent networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.cell.com/cms/attachment/560384/4035815/grbox4.jpg\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### linear factor models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=\"red\">In this chapter, we describe some of the simplest probabilistic models with latent variables: linear factor models</font>. \n",
    "     - These models are sometimes used as building blocks of \n",
    "         - mixture models or \n",
    "             <img src=\"http://www.vtkjournal.org/download/logopublication/4876/big\" width=400 />\n",
    "             <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/28/Bayesian-gaussian-mixture.svg/300px-Bayesian-gaussian-mixture.svg.png\" width=400 />\n",
    "         - larger, deep probabilistic models.\n",
    "             - 참고 : \n",
    "                 - [2] First Workshop on Deep Probabilistic Models -  http://ml.dcs.shef.ac.uk/DeepWorkshop/\n",
    "                 - [3] A Statistical View of Deep Learning (VI): What is Deep? - http://blog.shakirm.com/2015/06/a-statistical-view-of-deep-learning-vi-what-is-deep/\n",
    "                 <img src=\"https://allenlu2007.files.wordpress.com/2015/10/mlenewimage55.png?w=587&h=423\" width=400 />\n",
    "                 <img src=\"http://blog.shakirm.com/wp-content/uploads/2015/06/SVDL6.png\" width=400 />\n",
    "             \n",
    "     - They also show many of the basic approaches necessary \n",
    "         - to build generative models that \n",
    "             - the more advanced deep models will extend further.\n",
    "             <img src=\"http://images.slideplayer.com/15/4559668/slides/slide_8.jpg\" width=400 />\n",
    "             <img src=\"https://i.imgur.com/SOjew3N.png\" width=400 />\n",
    "             <img src=\"https://i.imgur.com/7JJxhT7.png\" width=400 />\n",
    "             <img src=\"https://i.imgur.com/JNFgTKR.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### linear decoder(=use of a stochastic, linear decoder function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear factor model is deﬁned by the use of a stochastic, linear decoderfunction that generates $x$ by adding noise to a linear transformation of $h$.\n",
    "* These models are interesting \n",
    "    - because they allow us to discover <font color=\"red\">explanatory factors</font> that have <font color=\"blue\">a simple joint distribution</font>. \n",
    "* The simplicity of using a linear decoder made these models some of the ﬁrst latent variable models to be extensively studied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### linear factor model (data geneartion process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap13.1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap13.2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap13.3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.1 Probabilistic PCA and Factor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [4] <font color=\"red\">Probabilistic PCA, EM, and more</font> - http://www.slideshare.net/hsharmasshare/probabilistic-pca-em-and-more\n",
    "* [5] 머피's 머신러닝: Latent Linear Model - http://www.slideshare.net/JungkyuLee1/s-latent-linear-model\n",
    "* [6] <font color=\"red\">Lecture 7 (prelude)  Some linear generative models and a coding perspective</font> - https://www.cs.toronto.edu/~hinton/csc2515/notes/lec7pre.ppt \n",
    "* [7] Ch 12. Continuous Latent Variables 12.2 ~ 12.4 (2006) - https://bi.snu.ac.kr/SEMINAR/ML/Bishop_PRML/PRML_ch12_sec2_4.ppt\n",
    "* [8] Ch 12. Continuous Latent Variables (2007) - https://bi.snu.ac.kr/SEMINAR/ML/Bishop_PRML/PRML_ch12_dycho.ppt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.cs.ubc.ca/~murphyk/Bayes/Figures/gmka.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://weigend.com/files/teaching/stanford/2008/stanford2008.wikispaces.com/file/view/pca_example.gif\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://images.slideplayer.com/16/5060808/slides/slide_51.jpg\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://images.slideplayer.com/1/234500/slides/slide_54.jpg\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://image.slidesharecdn.com/part-2-unsupervised-learning-machine-learning-techniques4986/95/part-2-unsupervised-learning-machine-learning-techniques-53-728.jpg?cb=1272283908\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://image.slidesharecdn.com/7-140222021421-phpapp02/95/s-latent-linear-model-5-638.jpg?cb=1393352299\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap_fa.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap_ppca.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic PCA (principal components analysis), factor analysis and other linear factor models are \n",
    "* special cases of the above equations (13.1 and 13.2) and \n",
    "* only differ in the choices made for \n",
    "    - the <font color=\"red\">noise distribution</font> and \n",
    "    - the model’s <font color=\"red\">prior</font> over latent variables $h$ before observing $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### factor analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap13.4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 참고 : \n",
    "<img src=\"http://www.holehouse.org/mlclass/15_Anomaly_Detection_files/Image%20[28].png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The role of the latent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The role of the latent variables is thus to <font color=\"red\">capture the dependencies</font> betweenthe diﬀerent observed variables $x_i$. Indeed, it can easily be shown that $x$ is just a multivariate normal random variable, with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap13.5.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probabilistic PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap13.6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://images.slideplayer.com/16/5060808/slides/slide_51.jpg\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reconstruction error $σ^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This probabilistic PCA model takes advantage of the observation that \n",
    "* most variations in the data can be captured by the latent variables $h$, \n",
    "* up to some small residual reconstruction error $σ^2$. \n",
    "* As shown by Tipping and Bishop (1999),probabilistic PCA becomes PCA as $σ →0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.2 Independent Component Analysis (ICA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.3 Slow Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap13.7.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap13.8.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap13.9.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.4 Sparse Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap13.10.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap13.11.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap13.12.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap13.13.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap13.14.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.5 Manifold Interpretation of PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap13.15.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap13.16.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap13.17.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap13.18.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap13.19.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap13.20.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* [1] 13 Linear Factor Models (DeepLearning Book) - http://www.deeplearningbook.org/contents/linear_factors.html\n",
    "* [2] First Workshop on Deep Probabilistic Models -  http://ml.dcs.shef.ac.uk/DeepWorkshop/\n",
    "* [3] A Statistical View of Deep Learning (VI): What is Deep? - http://blog.shakirm.com/2015/06/a-statistical-view-of-deep-learning-vi-what-is-deep/\n",
    "* [4] Probabilistic PCA, EM, and more - http://www.slideshare.net/hsharmasshare/probabilistic-pca-em-and-more\n",
    "* [5] 머피's 머신러닝: Latent Linear Model - http://www.slideshare.net/JungkyuLee1/s-latent-linear-model\n",
    "* [6] Lecture 7 (prelude)  Some linear generative models and a coding perspective - https://www.cs.toronto.edu/~hinton/csc2515/notes/lec7pre.ppt \n",
    "* [7] Ch 12. Continuous Latent Variables 12.2 ~ 12.4 (2006) - https://bi.snu.ac.kr/SEMINAR/ML/Bishop_PRML/PRML_ch12_sec2_4.ppt\n",
    "* [8] Ch 12. Continuous Latent Variables (2007) - https://bi.snu.ac.kr/SEMINAR/ML/Bishop_PRML/PRML_ch12_dycho.ppt\n",
    "* [9] Lecture 9: Continuous Latent Variable Models - https://www.cs.toronto.edu/~hinton/csc2515/notes/lec7middle.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
