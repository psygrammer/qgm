{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12. Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 싸이그래머 / QGM : 파트 4 - 딥러닝 [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* 12.1 Large Scale Deep Learning\n",
    "    - 12.1.1 Fast CPU Implementations\n",
    "    - 12.1.2 GPU Implementations\n",
    "    - 12.1.3 Large Scale Distributed Implementations\n",
    "    - 12.1.4 Model Compression\n",
    "    - 12.1.5 Dynamic Structure\n",
    "    - 12.1.6 Specialized Hardware Implementations of Deep Networks\n",
    "* 12.2 Computer Vision\n",
    "    - 12.2.1 Preprocessing\n",
    "        - 12.2.1.1 Contrast Normalization\n",
    "        - 12.2.1.2 Dataset Augmentation\n",
    "* 12.3 Speech Recognition\n",
    "* 12.4 Natural Language Processing\n",
    "    - 12.4.1 n-grams\n",
    "    - 12.4.2 Neural Language Models\n",
    "    - 12.4.3 High-Dimensional Outputs\n",
    "        - 12.4.3.1 Use of a Short List\n",
    "        - 12.4.3.2 Hierarchical Softmax\n",
    "        - 12.4.3.3 Importance Sampling\n",
    "        - 12.4.3.4 Noise-Contrastive Estimation and Ranking Loss\n",
    "    - 12.4.4 Combining Neural Language Models with n-grams\n",
    "    - 12.4.5 Neural Machine Translation\n",
    "        - 12.4.5.1 Using an Attention Mechanism and Aligning Pieces of Data\n",
    "    - 12.4.6 Historical Perspective\n",
    "* 12.5 Other Applications\n",
    "    - 12.5.1 Recommender Systems\n",
    "        - 12.5.1.1 Exploration Versus Exploitation\n",
    "        - 12.5.2 Knowledge Representation, Reasoning and Question Answering\n",
    "            - 12.5.2.1 Knowledge, Relations and Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we describe \n",
    "* how to use deep learning to solve applications in \n",
    "    - computer vision, \n",
    "    - speech recognition, \n",
    "    - natural language processing, and \n",
    "    - other applicationareas of commercial interest. \n",
    "* We begin by discussing \n",
    "    - the large scale neuralnetwork implementations \n",
    "        - required for most serious AI applications. \n",
    "* Next, we review several speciﬁc application areas that \n",
    "    - deep learning has been used to solve. \n",
    "    - While one goal of deep learning is to design algorithms that are capable of solving a broad variety of tasks, so far some degree of specialization is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.1 Large Scale Deep Learning\n",
    "* 12.1.1 Fast CPU Implementations\n",
    "* 12.1.2 GPU Implementations\n",
    "* 12.1.3 Large Scale Distributed Implementations\n",
    "* 12.1.4 Model Compression\n",
    "* 12.1.5 Dynamic Structure\n",
    "* 12.1.6 Specialized Hardware Implementations of Deep Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning is based on the philosophy of connectionism: \n",
    "* while an individual biological neuron or \n",
    "* an individual feature in a machine learning model is\n",
    "    - not intelligent, \n",
    "* a large population of these neurons or \n",
    "* features acting together can \n",
    "    - exhibit intelligent behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">It truly is important to emphasize the fact that the number of neurons must be large</font>. \n",
    "* One of the key factors responsible for the improvement in neural network’s accuracy and the improvement of the complexity of tasks they can solve between the 1980s and today is the dramatic increase in the size of the networks we use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://nbviewer.jupyter.org/github/songorithm/ML/blob/master/part1/study01/dml01/figures/fig1.6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://nbviewer.jupyter.org/github/songorithm/ML/blob/master/part1/study01/dml01/figures/fig1.7.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1.1 Fast CPU Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditionally, neural networks were trained using the CPU of a single machine.\n",
    "* Today, this approach is generally considered insuﬃcient. \n",
    "* We now mostly use \n",
    "    - GPU computing or \n",
    "    - the CPUs of many machines networked together. \n",
    "* Before moving to these expensive setups, \n",
    "    - researchers worked hard to demonstrate that \n",
    "        - CPUs could not manage \n",
    "            - the high computational workload \n",
    "                - required by neural networks.\n",
    "* A description of how to implement eﬃcient numerical CPU code is beyond the scope of this book, but <font color=\"red\">we emphasize here that careful implementation for speciﬁc CPU families can yield large improvements</font>.\n",
    "    - For example, in 2011, the best CPUs available could run neural network workloads faster \n",
    "        - when using ﬁxed-point arithmetic \n",
    "            - rather than ﬂoating-point arithmetic. \n",
    "    - By creating a carefully tuned ﬁxed-point implementation,\n",
    "        - Vanhoucke et al. (2011) \n",
    "        - obtained a 3×speedup over a strong ﬂoating-point system.\n",
    "* Other strategies, \n",
    "    - besides choosing whether to use \n",
    "        - ﬁxed or \n",
    "        - ﬂoating point,\n",
    "    - include \n",
    "        - optimizing data structures \n",
    "            - to avoid cache misses \n",
    "        - and using vector instructions. \n",
    "* <font color=\"red\">Many machine learning researchers neglect these implementation details</font>, but when the performance of an implementation restricts\n",
    "    - the size of the model, \n",
    "    - the accuracy of the model suﬀers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1.2 GPU Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [2] Training ConvNets in practice : Data augmentation, transfer learning, Distributed training, CPU/GPU bottlenecks, Efficient convolutions - http://cs231n.stanford.edu/slides/winter1516_lecture11.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most modern neural network implementations are based on graphics processing units. Graphics processing units (GPUs) are specialized hardware components that were originally developed for graphics applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Neural networks usually involve large and numerous buﬀers of\n",
    "    - parameters, \n",
    "    - activation values, and \n",
    "    - gradient values,\n",
    "    - each of which must be completely updated \n",
    "        - during every step of training. \n",
    "* These buﬀers are large enough to fall outside \n",
    "    - the cache of a traditional desktop computer \n",
    "        - so the memory bandwidth of the system often \n",
    "            - becomes the rate limiting factor.\n",
    "* GPUs oﬀer a compelling advantage over CPUs \n",
    "    - due to their high memory bandwidth.\n",
    "* Neural network training algorithms typically do not involve \n",
    "    - much branching or \n",
    "    - sophisticated control, \n",
    "    - so they are appropriate for GPU hardware. \n",
    "* Since neural networks can be divided into \n",
    "    - multiple individual “neurons” that \n",
    "        - can be processed independently \n",
    "            - from the other neurons in the same layer, \n",
    "    - neural networks easily beneﬁt from \n",
    "        - <font color=\"red\">the parallelism of GPU computing</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the diﬃculty of writing high performance GPU code, researchers should structure their workﬂow to avoid needing to write new GPU code in order to test new models or algorithms.\n",
    "* Typically, one can do this by building a software library of \n",
    "    - high performance operations like \n",
    "        - convolution and \n",
    "        - matrix multiplication, \n",
    "    - then specifying models in terms of calls to this library of operations.\n",
    "        - Pylearn2\n",
    "        - Theano\n",
    "        - cuda-convnet\n",
    "        - TensorFlow\n",
    "        - Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1.3 Large Scale Distributed Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [3] Toward Scalable Deep Learning -  http://mlcenter.postech.ac.kr/files/attach/workshop_fall_2015/%EC%84%9C%EC%9A%B8%EB%8C%80%ED%95%99%EA%B5%90_%EC%9C%A4%EC%84%B1%EB%A1%9C_%EA%B5%90%EC%88%98_v1.pdf\n",
    "* [4] Large Scale Distributed Deep Networks - http://www.slideshare.net/HiroyukiVincentYamaz/large-scale-distributed-deep-networks\n",
    "* [5] Large Scale Deep Learning Jeff Dean - http://www.slideshare.net/hustwj/cikm-keynotenov2014\n",
    "* [6] DeepDist : Lightning-Fast Deep Learning on Spark Via parallel stochastic gradient updates - http://deepdist.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, the computational resources available on a single machine areinsuﬃcient. We therefore want to distribute the workload of training and inference across many machines\n",
    "<img src=\"http://docplayer.net/docs-images/26/7315497/images/71-0.png\" width=600 />\n",
    "* <font color=\"red\">data parallelism</font>\n",
    "    - Distributing inference is simple, because each input example we want to processcan be run by a separate machine. This is known as data parallelism\n",
    "    - Data parallelism during training is somewhat harder\n",
    "    - We can increase the sizeof the minibatch used for a single SGD step, but usually we get less than linear\n",
    "    - It would be better to allow multiple machines to compute multiple gradient descent steps in parallel.\n",
    "        - Unfortunately,the standard deﬁnition of gradient descent is as a completely sequential algorithm\n",
    "        - This can be solved using asynchronous stochastic gradient descent\n",
    "    - asynchronous stochastic gradient descent\n",
    "        - shared memory (one machine)\n",
    "            - In this approach, several processor cores \n",
    "                - share the memory representing the parameters. \n",
    "            - Each core reads parameters without alock, \n",
    "                - then computes a gradient, \n",
    "                - then increments the parameters \n",
    "                    - without a lock \n",
    "        - parameter server (multi-machines)\n",
    "        <img src=\"http://image.slidesharecdn.com/presentationslides-160107181851/95/large-scale-distributed-deep-networks-32-638.jpg?cb=1452191027\" width=600 />\n",
    "* <font color=\"red\">model parallelism</font>\n",
    "    - It is also possible to get model parallelism, where multiple machines work together on a single data point, with each machine running a diﬀerent part of the model. This is feasible for both inference and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1.4 Model Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [7] Techniques for Efficient Implementation of Deep Neural Networks - http://www.slideshare.net/embeddedvision/techniques-for-efficient-implementation-of-deep-neural-networks-a-presentation-from-stanford\n",
    "* [8] Compressing CNN for Mobile Device - http://mlcenter.postech.ac.kr/files/attach/workshop_fall_2015/%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90_%EA%B9%80%EC%9A%A9%EB%8D%95_%EB%B0%95%EC%82%AC.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many commercial applications, it is much more important that the time and memory cost of \n",
    "* <font color=\"red\">running inference</font> in a machine learning model be low \n",
    "    - than that the time and memory cost of <font color=\"blue\">training</font> be low.\n",
    "* For applications that do not require personalization, \n",
    "    - it is possible to train a model once, \n",
    "    - then deploy it to be used bybillions of users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">A key strategy for reducing the cost of inference is model compression</font>\n",
    "* The basic idea of model compression is \n",
    "    - to <font color=\"blue\">replace</font> the original, <font color=\"blue\">expensive model</font> with \n",
    "    - a <font color=\"green\">smaller model</font> \n",
    "        - that requires less memory and runtime to store and evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These large models learn some functionf(x), but do so using many moreparameters than are necessary for the task. Their size is necessary only due to the limited number of training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1.5 Dynamic Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One strategy for <font color=\"red\">accelerating data processing systems</font> in general is to build systems that have <font color=\"red\">dynamic structure</font> in the <font color=\"blue\">graph describing the computation needed to process an input</font>.\n",
    "* Data processing systems can dynamically determine which subset of many neural networks should be run on a given input. \n",
    "* Individual neural networks can also exhibit dynamic structure internally by determining which subsetof features (hidden units) to compute given information from the input. \n",
    "* This form of dynamic structure inside neural networks is sometimes called <font color=\"red\">conditional computation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic structure of computations is a basic computer science principle appliedgenerally throughout the software engineering discipline. \n",
    "* The simplest versions of dynamic structure applied to neural networks are based on determining which subset of some group of neural networks (or other machine learning models) should be applied to a particular input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A venerable strategy for accelerating inference in a classiﬁer is to use a <font color=\"red\">cascade of classiﬁers</font>. \n",
    "* The cascade strategy may be applied when the goal is to detect the presence of a rare object (or event).\n",
    "* Violaand Jones (2001) used a cascade of boosted decision trees to implement a fast androbust face detector suitable for use in handheld digital cameras.\n",
    "    <img src=\"http://image.slidesharecdn.com/chernodubkharkivaiclubv03post-150701123201-lva1-app6891/95/details-of-lazy-deep-learning-for-images-recognition-in-zz-photo-app-41-638.jpg?cb=1435754072\" width=600 />\n",
    "* Another version of cascadesuses the earlier models to implement a sort of <font color=\"red\">hard attention mechanism</font>\n",
    "    <img src=\"http://cdn-ak.f.st-hatena.com/images/fotolife/P/PDFangeltop1/20160205/20160205113146.png\" width=600 />\n",
    "    <img src=\"https://qph.is.quoracdn.net/main-qimg-148b032baba54d1db01e3b8c39168f3a?convert_to_webp=true\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Decision trees</font> themselves are an example of dynamic structure, because eachnode in the tree determines which of its subtrees should be evaluated for each input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.time-management-guide.com/images/decision-tree.gif\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same spirit, one can use a neural network, called the gater to select which one out of several expert networks will be used to compute the output, given the current input.\n",
    "* The ﬁrst version of this idea is called the <font color=\"red\">mixture of experts</font> (Nowlan,1990; Jacobs et al., 1991), in which the gater outputs a set of probabilities orweights (obtained via a softmax nonlinearity), one per expert, and the ﬁnal output is obtained by the weighted combination of the output of the experts. \n",
    "* In that case, the use of the gater does not oﬀer a reduction in computational cost, but if a single expert is chosen by the gater for each example, we obtain the <font color=\"red\">hard mixtureof experts</font> (Collobert et al., 2001, 2002), which can considerably accelerate training and inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.frontiersin.org/files/Articles/119429/fnhum-08-00971-HTML/image_m/fnhum-08-00971-g004.jpg\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another kind of dynamic structure is a switch, where a hidden unit can receive input from diﬀerent units depending on the context. This <font color=\"red\">dynamic routing</font> approach can be interpreted as an attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://mlcenter.postech.ac.kr/files/attach/images/436/279/374/de62e0e6f848f9b5ded7c789c35483d4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">One major obstacle to using dynamically structured systems is the decreased degree of parallelism that results from the system following diﬀerent code branches for diﬀerent inputs</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1.6 Specialized Hardware Implementations of Deep Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.2 Computer Vision\n",
    "* 12.2.1 Preprocessing\n",
    "    - 12.2.1.1 Contrast Normalization\n",
    "    - 12.2.1.2 Dataset Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [11] ImageNet http://image-net.org/\n",
    "* [12] ILSVRC 2015 - http://image-net.org/challenges/ilsvrc+mscoco2015\n",
    "* [13] ILSVRC 2014 - http://image-net.org/challenges/LSVRC/2014/eccv2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2.1 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [14] Intro to Computer Vision, historical context - http://cs231n.stanford.edu/slides/winter1516_lecture1.pdf\n",
    "* [2] Training ConvNets in practice : Data augmentation, transfer learning, Distributed training, CPU/GPU bottlenecks, Efficient convolutions - http://cs231n.stanford.edu/slides/winter1516_lecture11.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computer vision usually requires relatively little of this kind of preprocessing.\n",
    "* pixel value range\n",
    "    - The images should be standardized so that their pixels all lie in the same,reasonable range, like [0,1] or [-1, 1]. \n",
    "    - Mixing images that lie in [0,1] with imagesthat lie in [0, 255] will usually result in failure.\n",
    "    - <font color=\"red\">Formatting images to have the samescale is the only kind of preprocessing that is strictly necessary</font>.\n",
    "* image size \n",
    "    - Many computer vision architectures require images of a standard size, so images must be cropped or scaled to ﬁt that size.\n",
    "    - <font color=\"blue\">However, even this rescaling is not always strictly necessary</font>.\n",
    "        - Some convolutional models accept variably-sized inputs and dynamically adjustthe size of their pooling regions to keep the output size constant.\n",
    "        - Other convolutional models have variable-sized output that automaticallyscales in size with the input, such as models that denoise or label each pixel in animage\n",
    "* Dataset augmentation\n",
    "    - Dataset augmentation may be seen as a way of <font color=\"red\">preprocessing the training set only</font>.\n",
    "    - Dataset augmentation is an excellent way to <font color=\"red\">reduce the generalization error</font> of most computer vision models.\n",
    "    -  A related idea applicable at test time is to show the model many diﬀerent versions of the same input \n",
    "        - (for example, the same image cropped at slightly diﬀerent locations) \n",
    "    - and have the diﬀerent instantiations of the model vote to determine the output. \n",
    "    - This latter idea can be interpreted as an <font color=\"red\">ensemble approach</font>, \n",
    "    - and helps to reduce generalization error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.1.1 Contrast Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.1.2 Dataset Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.3 Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.4 Natural Language Processing\n",
    "* 12.4.1 n-grams\n",
    "* 12.4.2 Neural Language Models\n",
    "* 12.4.3 High-Dimensional Outputs\n",
    "* 12.4.4 Combining Neural Language Models with n-grams\n",
    "* 12.4.5 Neural Machine Translation\n",
    "* 12.4.6 Historical Perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4.1 n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.7.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.8.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4.2 Neural Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.9.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4.3 High-Dimensional Outputs\n",
    "* 12.4.3.1 Use of a Short List\n",
    "* 12.4.3.2 Hierarchical Softmax\n",
    "* 12.4.3.3 Importance Sampling\n",
    "* 12.4.3.4 Noise-Contrastive Estimation and Ranking Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.10.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.3.1 Use of a Short List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.11.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.3.2 Hierarchical Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.12.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.3.3 Importance Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.13.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.14.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.15.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.3.4 Noise-Contrastive Estimation and Ranking Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.16.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4.4 Combining Neural Language Models with n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4.5 Neural Machine Translation\n",
    "* 12.4.5.1 Using an Attention Mechanism and Aligning Pieces of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.17.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.5.1 Using an Attention Mechanism and Aligning Pieces of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.18.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4.6 Historical Perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.5 Other Applications\n",
    "* 12.5.1 Recommender Systems\n",
    "    - 12.5.1.1 Exploration Versus Exploitation\n",
    "    - 12.5.2 Knowledge Representation, Reasoning and Question Answering        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.5.1 Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.19.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.5.1.1 Exploration Versus Exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.5.2 Knowledge Representation, Reasoning and Question Answering\n",
    "* 12.5.2.1 Knowledge, Relations and Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.5.2.1 Knowledge, Relations and Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.20.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1] deeplearning book - http://www.deeplearningbook.org\n",
    "* [2] Training ConvNets in practice : Data augmentation, transfer learning, Distributed training, CPU/GPU bottlenecks, Efficient convolutions - http://cs231n.stanford.edu/slides/winter1516_lecture11.pdf\n",
    "* [3] Toward Scalable Deep Learning -  http://mlcenter.postech.ac.kr/files/attach/workshop_fall_2015/%EC%84%9C%EC%9A%B8%EB%8C%80%ED%95%99%EA%B5%90_%EC%9C%A4%EC%84%B1%EB%A1%9C_%EA%B5%90%EC%88%98_v1.pdf\n",
    "* [4] Large Scale Distributed Deep Networks - http://www.slideshare.net/HiroyukiVincentYamaz/large-scale-distributed-deep-networks\n",
    "* [5] Large Scale Deep Learning Jeff Dean - http://www.slideshare.net/hustwj/cikm-keynotenov2014\n",
    "* [6] DeepDist : Lightning-Fast Deep Learning on Spark Via parallel stochastic gradient updates - http://deepdist.com/\n",
    "* [7] Techniques for Efficient Implementation of Deep Neural Networks - http://www.slideshare.net/embeddedvision/techniques-for-efficient-implementation-of-deep-neural-networks-a-presentation-from-stanford\n",
    "* [8] Compressing CNN for Mobile Device - http://mlcenter.postech.ac.kr/files/attach/workshop_fall_2015/%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90_%EA%B9%80%EC%9A%A9%EB%8D%95_%EB%B0%95%EC%82%AC.pdf\n",
    "* [9] ATTENTION MECHANISM - https://blog.heuritech.com/2016/01/20/attention-mechanism/\n",
    "* [10] Dynamic Scheduler for Scaling up Deep Learning - http://mlcenter.postech.ac.kr/deep_learning\n",
    "* [11] ImageNet http://image-net.org/\n",
    "* [12] ILSVRC 2015 -  http://image-net.org/challenges/ilsvrc+mscoco2015\n",
    "* [13] ILSVRC 2014 - http://image-net.org/challenges/LSVRC/2014/eccv2014\n",
    "* [14] Intro to Computer Vision, historical context - http://cs231n.stanford.edu/slides/winter1516_lecture1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
