{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12. Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 싸이그래머 / QGM : 파트 4 - 딥러닝 [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* 12.1 Large Scale Deep Learning\n",
    "    - 12.1.1 Fast CPU Implementations\n",
    "    - 12.1.2 GPU Implementations\n",
    "    - 12.1.3 Large Scale Distributed Implementations\n",
    "    - 12.1.4 Model Compression\n",
    "    - 12.1.5 Dynamic Structure\n",
    "    - 12.1.6 Specialized Hardware Implementations of Deep Networks\n",
    "* 12.2 Computer Vision\n",
    "    - 12.2.1 Preprocessing\n",
    "        - 12.2.1.1 Contrast Normalization\n",
    "        - 12.2.1.2 Dataset Augmentation\n",
    "* 12.3 Speech Recognition\n",
    "* 12.4 Natural Language Processing\n",
    "    - 12.4.1 n-grams\n",
    "    - 12.4.2 Neural Language Models\n",
    "    - 12.4.3 High-Dimensional Outputs\n",
    "        - 12.4.3.1 Use of a Short List\n",
    "        - 12.4.3.2 Hierarchical Softmax\n",
    "        - 12.4.3.3 Importance Sampling\n",
    "        - 12.4.3.4 Noise-Contrastive Estimation and Ranking Loss\n",
    "    - 12.4.4 Combining Neural Language Models with n-grams\n",
    "    - 12.4.5 Neural Machine Translation\n",
    "        - 12.4.5.1 Using an Attention Mechanism and Aligning Pieces of Data\n",
    "    - 12.4.6 Historical Perspective\n",
    "* 12.5 Other Applications\n",
    "    - 12.5.1 Recommender Systems\n",
    "        - 12.5.1.1 Exploration Versus Exploitation\n",
    "        - 12.5.2 Knowledge Representation, Reasoning and Question Answering\n",
    "            - 12.5.2.1 Knowledge, Relations and Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we describe \n",
    "* how to use deep learning to solve applications in \n",
    "    - computer vision, \n",
    "    - speech recognition, \n",
    "    - natural language processing, and \n",
    "    - other applicationareas of commercial interest. \n",
    "* We begin by discussing \n",
    "    - the large scale neuralnetwork implementations \n",
    "        - required for most serious AI applications. \n",
    "* Next, we review several speciﬁc application areas that \n",
    "    - deep learning has been used to solve. \n",
    "    - While one goal of deep learning is to design algorithms that are capable of solving a broad variety of tasks, so far some degree of specialization is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.1 Large Scale Deep Learning\n",
    "* 12.1.1 Fast CPU Implementations\n",
    "* 12.1.2 GPU Implementations\n",
    "* 12.1.3 Large Scale Distributed Implementations\n",
    "* 12.1.4 Model Compression\n",
    "* 12.1.5 Dynamic Structure\n",
    "* 12.1.6 Specialized Hardware Implementations of Deep Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [2] Training ConvNets in practice : Data augmentation, transfer learning, Distributed training, CPU/GPU bottlenecks, Efficient convolutions - http://cs231n.stanford.edu/slides/winter1516_lecture11.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning is based on the philosophy of connectionism: \n",
    "* while an individual biological neuron or \n",
    "* an individual feature in a machine learning model is\n",
    "    - not intelligent, \n",
    "* a large population of these neurons or \n",
    "* features acting together can \n",
    "    - exhibit intelligent behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">It truly is important to emphasize the fact that the number of neurons must be large</font>. \n",
    "* One of the key factors responsible for the improvement in neural network’s accuracy and the improvement of the complexity of tasks they can solve between the 1980s and today is the dramatic increase in the size of the networks we use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://nbviewer.jupyter.org/github/songorithm/ML/blob/master/part1/study01/dml01/figures/fig1.6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://nbviewer.jupyter.org/github/songorithm/ML/blob/master/part1/study01/dml01/figures/fig1.7.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1.1 Fast CPU Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Traditionally, neural networks were trained using the CPU of a single machine.\n",
    "    - Today, this approach is generally considered insuﬃcient. \n",
    "* We now mostly use \n",
    "    - GPU computing or \n",
    "    - the CPUs of many machines networked together. \n",
    "* Before moving to these expensive setups, \n",
    "    - researchers worked hard to demonstrate that \n",
    "        - CPUs could not manage \n",
    "            - the high computational workload \n",
    "                - required by neural networks.\n",
    "* A description of how to implement eﬃcient numerical CPU code is beyond the scope of this book, but <font color=\"red\">we emphasize here that careful implementation for speciﬁc CPU families can yield large improvements</font>.\n",
    "    - For example, in 2011, the best CPUs available could run neural network workloads faster \n",
    "        - when using ﬁxed-point arithmetic \n",
    "            - rather than ﬂoating-point arithmetic. \n",
    "    - By creating a carefully tuned ﬁxed-point implementation,\n",
    "        - Vanhoucke et al. (2011) \n",
    "        - obtained a 3×speedup over a strong ﬂoating-point system.\n",
    "* Other strategies, \n",
    "    - besides choosing whether to use \n",
    "        - ﬁxed or \n",
    "        - ﬂoating point,\n",
    "    - include \n",
    "        - optimizing data structures \n",
    "            - to avoid cache misses \n",
    "        - and using vector instructions. \n",
    "* <font color=\"red\">Many machine learning researchers neglect these implementation details</font>, but when the performance of an implementation restricts\n",
    "    - the size of the model, \n",
    "    - the accuracy of the model suﬀers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1.2 GPU Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Most modern neural network implementations are based on graphics processing units. Graphics processing units (GPUs) are specialized hardware components that were originally developed for graphics applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Neural networks usually involve large and numerous buﬀers of\n",
    "    - parameters, \n",
    "    - activation values, and \n",
    "    - gradient values,\n",
    "    - each of which must be completely updated \n",
    "        - during every step of training. \n",
    "* These buﬀers are large enough to fall outside \n",
    "    - the cache of a traditional desktop computer \n",
    "        - so the memory bandwidth of the system often \n",
    "            - becomes the rate limiting factor.\n",
    "* GPUs oﬀer a compelling advantage over CPUs \n",
    "    - due to their high memory bandwidth.\n",
    "* Neural network training algorithms typically do not involve \n",
    "    - much branching or \n",
    "    - sophisticated control, \n",
    "    - so they are appropriate for GPU hardware. \n",
    "* Since neural networks can be divided into \n",
    "    - multiple individual “neurons” that \n",
    "        - can be processed independently \n",
    "            - from the other neurons in the same layer, \n",
    "    - neural networks easily beneﬁt from \n",
    "        - <font color=\"red\">the parallelism of GPU computing</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the diﬃculty of writing high performance GPU code, researchers should structure their workﬂow to avoid needing to write new GPU code in order to test new models or algorithms.\n",
    "* Typically, one can do this by building a software library of \n",
    "    - high performance operations like \n",
    "        - convolution and \n",
    "        - matrix multiplication, \n",
    "    - then specifying models in terms of calls to this library of operations.\n",
    "        - Pylearn2\n",
    "        - Theano\n",
    "        - cuda-convnet\n",
    "        - TensorFlow\n",
    "        - Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1.3 Large Scale Distributed Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1.4 Model Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1.5 Dynamic Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1.6 Specialized Hardware Implementations of Deep Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.2 Computer Vision\n",
    "* 12.2.1 Preprocessing\n",
    "    - 12.2.1.1 Contrast Normalization\n",
    "    - 12.2.1.2 Dataset Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2.1 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.1.1 Contrast Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.1.2 Dataset Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.3 Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.4 Natural Language Processing\n",
    "* 12.4.1 n-grams\n",
    "* 12.4.2 Neural Language Models\n",
    "* 12.4.3 High-Dimensional Outputs\n",
    "* 12.4.4 Combining Neural Language Models with n-grams\n",
    "* 12.4.5 Neural Machine Translation\n",
    "* 12.4.6 Historical Perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4.1 n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.7.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.8.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4.2 Neural Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.9.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4.3 High-Dimensional Outputs\n",
    "* 12.4.3.1 Use of a Short List\n",
    "* 12.4.3.2 Hierarchical Softmax\n",
    "* 12.4.3.3 Importance Sampling\n",
    "* 12.4.3.4 Noise-Contrastive Estimation and Ranking Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.10.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.3.1 Use of a Short List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.11.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.3.2 Hierarchical Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.12.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.3.3 Importance Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.13.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.14.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.15.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.3.4 Noise-Contrastive Estimation and Ranking Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.16.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4.4 Combining Neural Language Models with n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4.5 Neural Machine Translation\n",
    "* 12.4.5.1 Using an Attention Mechanism and Aligning Pieces of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.17.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.5.1 Using an Attention Mechanism and Aligning Pieces of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.18.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4.6 Historical Perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.5 Other Applications\n",
    "* 12.5.1 Recommender Systems\n",
    "    - 12.5.1.1 Exploration Versus Exploitation\n",
    "    - 12.5.2 Knowledge Representation, Reasoning and Question Answering        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.5.1 Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.19.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.5.1.1 Exploration Versus Exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.5.2 Knowledge Representation, Reasoning and Question Answering\n",
    "* 12.5.2.1 Knowledge, Relations and Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.5.2.1 Knowledge, Relations and Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.20.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1] deeplearning book - http://www.deeplearningbook.org\n",
    "* [2] Training ConvNets in practice : Data augmentation, transfer learning, Distributed training, CPU/GPU bottlenecks, Efficient convolutions - http://cs231n.stanford.edu/slides/winter1516_lecture11.pdf\n",
    "* [3] Toward Scalable Deep Learning -  http://mlcenter.postech.ac.kr/files/attach/workshop_fall_2015/%EC%84%9C%EC%9A%B8%EB%8C%80%ED%95%99%EA%B5%90_%EC%9C%A4%EC%84%B1%EB%A1%9C_%EA%B5%90%EC%88%98_v1.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
