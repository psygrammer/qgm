{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to connectionist models in cognitive psychology: Basic structures, processes, and algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 싸이그래머 / QGM : 파트 3 - 연결주의모형 [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* INTRODUCTION\n",
    "* THE NEURAL BASIS OF CONNECTIONIST MODELS\n",
    "* COGNITIVE AND NEURAL INTERPRETATIONS OF CONNECTIONIST NETWORKS\n",
    "* BASIC STRUCTURES AND PROCESSES OF CONNECTIONIST MODELS\n",
    "* LEARNING RULES\n",
    "* FURTHER EXTENSIONS OF THE DELTA RULE—MULTI-LAYER NETWORKS AND NONLINEAR MAPPINGS\n",
    "* FEEDBACK AND RECURRENT ARCHITECTURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고자료\n",
    "* [2] Introduction to Cognitive Science - http://users.metu.edu.tr/bozsahin/intro-cogsci/w1-history.ppt\n",
    "* [3] Connectionist Modeling  - http://people.umass.edu/alc/course_pages/fall_2004/modeling_behavior/lectures/connectionism.ppt \n",
    "* [4] Computationalism: The Very Idea - http://www.cs.bilkent.edu.tr/~david/papers/Computationalism.ppt\n",
    "* [5] Connectionist Approaches - http://www.slideshare.net/cheapiseth1/chapter-6-connectionist-approaches\n",
    "* [6] Connectionism vs. computationalism debate (wikipedia) - https://en.wikipedia.org/wiki/Connectionism#Connectionism_vs._computationalism_debate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This book aims to present an overview of the current state of connectionist modelling in cognitive psychology, covering a broad range of areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* connectionist (or “neural network”) models, PDP(Parallel Distributed Processing Model)\n",
    "* cognitive psychology\n",
    "* Connectionism vs. computationalism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE NEURAL BASIS OF CONNECTIONIST MODELS\n",
    "* The neuron\n",
    "* The action potential\n",
    "* Neuronal communication and synaptic integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The action potential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://faculty.pasadena.edu/dkwon/chap%208_files/images/image33.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuronal communication and synaptic integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://qbi.uq.edu.au/filething/get/38344/Neuron-synapse_brain-physiology_QBI.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://ib-biology2010-12.wikispaces.com/file/view/Synaptic_Integration.jpg/307656242/649x566/Synaptic_Integration.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGNITIVE AND NEURAL INTERPRETATIONS OF CONNECTIONIST NETWORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our knowledge of brain function is still at an early stage, and we should anticipate that there are many significant functional principles still to be discovered. It could still turn out that some currently cherished beliefs regard- ing the psychological significance of what we know about the brain (e.g. the role of synaptic change in memory formation) are mistaken. \n",
    "* <font color=\"red\">If such ideas proved wrong, what would be the consequences for the connectionist program?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Some might argue that in fact it would not matter, as connectionist cognitive models are first and foremost cognitive; i.e. they are to be judged only on how well they explain and predict behavioural data from cognitive psychology experiments.\n",
    "* From this point of view, <font color=\"red\">the basic elements of connectionist models are to be interpreted only in abstract functional terms</font>.\n",
    "* Connections and weights in a network represent patterns of functional interactivity between cognitive units, such that units that are cognitively related can activate each other, while units that are cognitively incompatible inhibit each other; it does not matter that the brain contains (modifiable) excitatory and inhibitory synapses—maybe the cognitively important task (generation of internally consistent states) is realized in some other way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nevertheless, I believe that most connectionist cognitive modellers, however rarely a new neuroscientific finding actually impinges on their work, would reject the logical consequence of this view, in which the coarse analogy between artificial and real neural networks is entirely accidental, and of no practical or theoretical importance. \n",
    "* <font color=\"red\">Rather, the general belief is that a neurally-grounded cognitive psychology is both desirable and achievable.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Connectionism is, at least in practice, a genuinely constraining framework, and the constraints lead to <font color=\"red\">fundamental theoretical questions</font> that also make sense to neuroscientists; e.g. \n",
    "    - how can cognition arise from interactions between basic processors capable of only simple analogue computations? \n",
    "    - How are memories formed in a system limited to changing patterns of connectivity between these simple elements? \n",
    "    - How does binding of distributed information take place to produce unitary percepts? \n",
    "    - How does a parallel, distributed system (without a serial, central processor) control its own behaviour, in space (attention) and time (serial order)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASIC STRUCTURES AND PROCESSES OF CONNECTIONIST MODELS\n",
    "* Units and activation levels\n",
    "* Vector representation of activation patterns\n",
    "* Representation of cognitive elements by units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keyword\n",
    "* node (= unit = neuron)\n",
    "* activation level,\n",
    "* weight\n",
    "* layer\n",
    "* patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Units and activation levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://i.stack.imgur.com/KUvpQ.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://homepages.gold.ac.uk/nikolaev/perceptr.gif\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector representation of activation patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation of cognitive elements by units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned several times above, the units in a network are used to represent the cognitive elements that are required in the domain being modelled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### local representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The simplest way to do this is to <font color=\"red\">assign one unit to each identifiable element</font>. \n",
    "* For instance, in a model incorporating a mental lexicon (store of words), \n",
    "    - <font color=\"red\">each known word</font> would be represented by one unit. * In a model of face processing, \n",
    "    - each unit would represent a <font color=\"red\">known face</font>.\n",
    "* This is known as a local representation, as to find out to what extent a given mental entity is <font color=\"red\">currently active we need only look “locally” at a single unit</font>.\n",
    "* orthogonality\n",
    "    - From a more formal point of view, each element is represented orthogonally to all the others.\n",
    "    - The activation pattern (vector) {1 0 0 0} would represent John, {0 1 0 0} would represent Paul, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### distributed representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=\"red\">Local representation</font> requires as many units as there are things to be represented, and has been criticized for (amongst other things) being <font color=\"red\">inefficient</font>. \n",
    "* An alternative is to use a <font color=\"blue\">distributed representation</font>.\n",
    "    - In this case, cognitive elements are represented not by the activation of individual units, but by <font color=\"blue\">the pattern of activation over a set of units</font>. \n",
    "    - In the above example, John might be {1 0 1 1}, Paul {1 1 0 0}, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local VS Distributed ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The relative advantages of local and distributed representations have been the cause of debate, with <font color=\"red\">some influential authors considering the use of distributed representations</font> to be defining feature of the connectionist approach (see Page, 2000, and associated commentaries). \n",
    "    - In practice, many models <font color=\"blue\">use both forms</font> of representation, and in many cases the <font color=\"blue\">use of distributed representations is restricted to the “hidden units”, in networks trained using back-propagation</font> or a related method (discussed below in the section on learning). \n",
    "* From a psychology perspective, \n",
    "    - it is best to treat representations as embodying substantive claims about cognitive representation. \n",
    "    - <font color=\"red\">Distributed representations can capture the similarity structure</font> of a domain.\n",
    "    - <font color=\"blue\">The central feature of local representation</font> is not really “locality” per se (i.e. all the activations in one unit), but the more <font color=\"blue\">abstract property of orthogonality, or independence</font>, mentioned earlier.\n",
    "        - That is, considered as activation vectors, any two local representations (over the same vector space) are orthogonal to (uncorrelated with) each other, so that any- thing that happens to one representation does not affect any other.\n",
    "        - At the word level, cat and sat are distinct and unrelated, and local (i.e. orthogonal) lexical representations capture this perfectly.\n",
    "    - <font color=\"red\">If the brain were to use only distributed representations based on similarity structure, then in essence it would have failed to capture a pervasive and behaviourally significant feature of the world</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connections and weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The matrix representation of sets of weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spread of activation and the net (summed) input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.7.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### net input\n",
    "As each individual input is the product of an activation and a weight [equation (1)], the net input is therefore a sum of products. The net input from a set of n units (1,2, . . ., n) to a unit j, netj, is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.6.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.8.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spread of activation using vector and matrix notations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.9.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation (3) gives the net input to just one output unit. The same process has to be carried out for all output units (using the appropriate weight vector, or row, from the matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happily, what has just been described corresponds to the multiplication of a vector (input pattern) by a matrix (weights). Hence, using vector–matrix notation, the whole process can be written:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.10.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rule for converting the input to a unit into an activation level is generally known as the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.11.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we list a few common examples also found in articles in this book. Each example is shown graphically in Fig. 1.7. In each case the graphs plot the net input to a unit (x axis) against its activation level (y axis)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Activation level is (linearly) proportional to the net input."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2. Binary threshold units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.12.png\" />"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3. Sigmoidal (S-shaped) activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.13.png\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial basis functions (RBFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation functions discussed above share the property that, as the net input to a unit becomes more positive (excitatory), the higher the expected activation value of the unit (the thresholded unit of course changes instantly once the threshold is reached, but as the net input increases, the more likely it is that the threshold will have been crossed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last class of rule we discuss, radial basis function, does not have this property. Instead, we can think of a unit as having an optimal pattern of input, and as the actual input moves away from the optimal pattern, the activation level of the unit decreases (Fig. 1.7). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.11.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation decay and temporal integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/95/Action_Potential.gif/600px-Action_Potential.gif\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4a/Action_potential.svg/600px-Action_potential.svg.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://d2gme0e5d9kd75.cloudfront.net/content/royfocus/1/1/75/F2.medium.gif\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEARNING RULES\n",
    "* Unsupervised learning—the Hebb rule\n",
    "* Supervised learning—the delta rule\n",
    "* The delta rule and the Hebb rule: A comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Learning, defined as the acquisition of knowledge and/or change in behaviour as a result of experience, is of great importance to connectionist models.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">How does one define what is a “better” response?</font> In general, “better” means that the interaction with the environment is improved; that successful predictions can be made about it, on the basis of partial information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### category (or classification) learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the case of category (or classification) learning, experience leads to clustering of stimuli based on similarity—in brief, similar things go together, and should be treated in a similar manner.\n",
    "    - For instance, if one has learned from prior experience that some parts of plants taste good (e.g. fruits), and some bad (e.g. leaves) then the ability to classify a new stimulus as a fruit or a leaf, just by looking at it, will determine how one responds to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### associative learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the case of associative learning, the goal of learning is to associate disparate aspects of experience that occur together, hence enabling us to predict the structure of our environment on the basis of partial cues.\n",
    "    - For instance, faces and voices are processed in different brain areas, yet through experience we associate particular voices with faces of people that we know. On hearing a known voice, we look for the presence of a particular face. If we see a friend, we immediately notice any change in his/her voice (maybe he/she has a cold), or accent (who has he/she been talking to?). Hence, one form of perceptual input gives rise to learned expectations about another, and allows us to predict aspects of our environment ahead of time (and to detect changes in them)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In neural networks, a broad distinction is made between supervised and unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### supervised learning\n",
    "* In supervised learning, the network is provided with some form of environmental feedback which indicates how well it is performing, and learning will only take place when the network performs badly, i.e. makes an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unsupervised learning\n",
    "* In the case of unsupervised learning, there is no explicit comparison between actual and desired outputs. Rather, the network typically has implicit goals, such as forming a useful categorization (grouping) of sets of input stimuli, or extract- ing the co-occurrence relationships amongst features of the environment, which permit the prediction of features not actually present (e.g. Hebbian learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we discuss in more detail some of the more commonly used learn- ing rules that occur in the articles in the current book. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">As stated, all learning rules change the weights between units.</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning—the Hebb rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [8] Plasticity and Learning - http://www.intsci.ac.cn/shizz/course/ni10.ppt\n",
    "* [9] Hebbian Learning - http://www.slideshare.net/mentelibre/hebbian-learning\n",
    "* [10] Lebbian Learing Rule - http://www.aistudy.co.kr/neural/hebbian_learning.htm\n",
    "* [11] neural-networks-12102680 - http://www.slideshare.net/balveenchugh/neural-networks-12102680"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest learning rule is named after the psychologist Donald Hebb, who formulated an intuitive, physiologically based version of it in a work pub- lished in 1949. Hebb proposed that knowledge was stored in “cell assemblies”, connected groups of neurons that would activate each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://image.slidesharecdn.com/2008-01-09-miagkikh-140212224414-phpapp02/95/learning-in-networks-were-pavlov-and-hebb-right-7-638.jpg?cb=1392245210\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://neuronaldynamics.epfl.ch/online/x589.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In connectionist terms, the basic rule has the form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.14.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where ai and aj are the activations of two units connected by the weight wij."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### learning rate\n",
    "* The rule can be added to in various ways which complicate its mathematical expression (and for this reason have been omitted). \n",
    "* For instance, a learning rate parameter is usually added, which scales the size of the weight change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long-Term Potentiation (LTP) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hence, the weight increases (i.e. has a positive change) whenever the two units are active at the same time (if either unit has zero activation, then clearly ∆ wij = 0, and the weight does not change). In neural terms, the rule can be thought of as stating that the strength of (excitatory) synapses should increase when- ever two neurons connected by such synapses fire (become depolarized) at the same time (or at least very closely in time).\n",
    "* The phenomenon of long-term potentiation (LTP) of synapses has this characteristic (see e.g. Carlson, 2001, Chapter 14). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### associative learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The rule can be used to perform associative learning between mental entities that are active at the same time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.15.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### anti-Hebb & Long-Term Depression (LTD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* More interestingly, the individual weights can be made to decrease actively by a so-called “anti-Hebb” rule, whereby the weight is decreased by some amount if the input unit (ai) is active but the receiving unit (aj) is inactive or only weakly active. In the example shown in Fig. 1.8, anti-Hebb learning would lead to a reduction in the weights (shown as dashed lines in the figure) from “C”, “A”, “T” to the word “Rat”, as “Rat” was not activated. \n",
    "* Interestingly, this proposal has physiological support in the phenomenon of long-term depression (LTD), in which firing of a pre-synaptic neuron combined with weak post-synaptic depolarization, or hyperpolariza- tion, leads to a reduction in synaptic strength."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning—the delta rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [12] Lecture 3: Delta Rule - http://www.cs.stir.ac.uk/courses/31YF/lectures/ANN/3-DeltaRule.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning, in contrast, only takes place when an error is detected in the output response of the network. For the error to be gener- ated, the actual response of the network has to be compared with an exter- nally provided “target” activation pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### error type\n",
    "* Errors can be of two types, which we will refer to as “omission” and “commission” errors. \n",
    "* In an omission error, \n",
    "    - the network fails to activate a node that should have come on.\n",
    "    - Omission errors lead to an increase in excitatory weights (and/or reduc- tion in inhibitory weights) to the output unit that was not active enough, from all the input units that where activated by the input stimulus.\n",
    "* In a commission error, \n",
    "    - the network activates a unit that should have remained inactive. \n",
    "    - Commission errors lead to a reduction in excitatory weights (and/or increase in inhibitory weights) to the inappropriately active unit from all the input units that where activated by the input stimulus. \n",
    "* The two types of error frequently co-occur. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Widrow–Hoff (or “delta”) rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.16.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The expression in brackets, \n",
    "    - targetj − actualj, is called the error term, \n",
    "    - and is computed very simply by subtracting the actual activation of the output unit from the target activation provided by the teacher. \n",
    "* The error term is then multiplied by the activation of the input unit from which the connection derives. \n",
    "    - Consequently, if the input unit is not active (ai = 0), then no weight change takes place. \n",
    "* Here we notice a similarity with the Hebb rule, discussed above—weight changes only take place when the “pre-synaptic neuron” is active. \n",
    "* Indeed, if we denote the error term dj, \n",
    "    - the <font color=\"red\">delta rule is dj × ai</font>, \n",
    "    - while the <font color=\"blue\">Hebb rule is aj × ai</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.17.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The delta rule and the Hebb rule: A comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to implement the delta rule without explicitly computing the error. We can see this by applying some simple algebra. We can rewrite the delta rule [equation (8)] by multiplying out the error term in the bracket by ai, which gives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.18.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rule now appears as two “Hebb-like” terms (actually, Hebb and anti-Hebb), with no direct comparison between a target and desired activation.\n",
    "* The first term makes weights more positive (or less negative), \n",
    "* while the second does the opposite (note the minus sign). \n",
    "* The weight change due to the first term is precisely the Hebb rule, if we imagine the target activations instantiated on the output units, while the input units are active. This will make the connections to the output units that should be ON more positive. \n",
    "* The second term can be computed by activating the input units and then decrementing the weights to any output units that are activated as a result. This is quite literally an anti-Hebb rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FURTHER EXTENSIONS OF THE DELTA RULE—MULTI-LAYER NETWORKS AND NONLINEAR MAPPINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The delta rule is defined for two-layer networks in which the input units are directly attached to the output units (Fig. 1.2a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### linearity & nonlinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the set of problems (defined as input–output mappings) that a two-layer network can solve in principle is limited. In technical terms, for a complete solution to be achievable by a two-layer net, the relationship defined by the mapping has to be linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://dmm613.files.wordpress.com/2014/12/non_linearly_separable.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hidden units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The addition of hidden units to networks permits them to handle arbitrary, nonlinear relationships (the hidden units must use a nonlinear activation function, such as the sigmoid, Fig. 1.7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The immediate problem with applying the delta rule is that, although it could be adopted to change the weights from the hidden to output units (the activation of the input units in equation (8) could be replaced with the activation of the hidden units), it provides no way of adapting the weights from the input to hidden units. In particular, if we want to use a supervised learning rule, we need a way to define the error for the hidden units, so that the input-to-hidden weights can be changed to reduce the error. But how can this be done, given that the target activations are only defined for the output units? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### back-propagation learning rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.19.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEEDBACK AND RECURRENT ARCHITECTURES\n",
    "* Within-layer feedback and winner-takes-all dynamics\n",
    "* Between-layer feedback and attractor dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Within-layer feedback and winner-takes-all dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.20.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.21.png\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Between-layer feedback and attractor dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고문헌\n",
    "* [1] Connectionist Models in Cognitive Psycholgy - http://www.amazon.com/Connectionist-Cognitive-Psychology-Studies-Cognition/dp/0415646901/\n",
    "* [2] Introduction to Cognitive Science - http://users.metu.edu.tr/bozsahin/intro-cogsci/w1-history.ppt\n",
    "* [3] Connectionist Modeling  - http://people.umass.edu/alc/course_pages/fall_2004/modeling_behavior/lectures/connectionism.ppt \n",
    "* [4] Computationalism: The Very Idea - http://www.cs.bilkent.edu.tr/~david/papers/Computationalism.ppt\n",
    "* [5] Connectionist Approaches - http://www.slideshare.net/cheapiseth1/chapter-6-connectionist-approaches\n",
    "* [6] Connectionism vs. computationalism debate (wikipedia) - https://en.wikipedia.org/wiki/Connectionism#Connectionism_vs._computationalism_debate\n",
    "* [7] Learning in Networks: were Pavlov and Hebb right? - http://www.slideshare.net/vmiagkikh/learning-in-networks-was-p\n",
    "* [8] Plasticity and Learning - http://www.intsci.ac.cn/shizz/course/ni10.ppt\n",
    "* [9] Hebbian Learning - http://www.slideshare.net/mentelibre/hebbian-learning\n",
    "* [10] Lebbian Learing Rule - http://www.aistudy.co.kr/neural/hebbian_learning.htm\n",
    "* [11] neural-networks-12102680 - http://www.slideshare.net/balveenchugh/neural-networks-12102680\n",
    "* [12] Lecture 3: Delta Rule - http://www.cs.stir.ac.uk/courses/31YF/lectures/ANN/3-DeltaRule.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
